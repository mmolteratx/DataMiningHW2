{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport xgboost as xgb\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, Lasso, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\n\n\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":3,"outputs":[{"output_type":"stream","text":"/kaggle/input/house-prices-advanced-regression/train.csv\n/kaggle/input/house-prices-advanced-regression/test.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Load data and combine input data into one DataFrame. Preprocessing from apapiu: https://www.kaggle.com/apapiu/regularized-linear-models"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/house-prices-advanced-regression/train.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression/test.csv\")\n\nall_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                      test.loc[:,'MSSubClass':'SaleCondition']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#log transform the target:\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n\nall_data = pd.get_dummies(all_data)\n\n#filling NA's with the mean of the column:\nall_data = all_data.fillna(all_data.mean())\n\n#creating matrices for sklearn:\nX_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]\ny = train.SalePrice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below follows problem 2.2. Output submitted to Kaggle. Results for alpha = 0.1 is rmse = 0.13565\n\n2.  Follow the data preprocessing steps from https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models.  Then run a ridge regression usingÎ±= 0.1.Make a submission of this prediction, what is the RMSE you get?(Hint:  remember to exponentiate np.expm1(ypred) your predictions)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)\n\nridge = Ridge(alpha=0.1)\nridge.fit(X_train, y)\n\nout = pd.DataFrame({\"SalePrice\":np.expm1(ridge.predict(X_test))})\ni = pd.DataFrame({\"Id\":test[\"Id\"]})\nout = pd.concat([i, out], axis=1)\nout.to_csv(\"Ridge0a1.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below follows problem 2.3. CV tuned Ridge and Lasso.\n\nLasso score: 0.12455\n\n3.  Compare a ridge regression and a lasso regression model.  Optimize the alphas using crossvalidation.  What is the best score you can get from a single ridge regression model and froma single lasso model?"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_ridge = RidgeCV(alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]).fit(X_train, y)\nprint(\"Ridge rmse mean: \", rmse_cv(model_ridge).mean())\nprint(\"Optimal Ridge Alpha: \", model_ridge.alpha_)\n\nmodel_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)\nprint(\"Lasso rmse mean: \", rmse_cv(model_lasso).mean())\nprint(\"Optimal Lasso Alpha: \", model_lasso.alpha_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below follows problem 2.4. Lasso coefficients with varying Alpha.\n\n4.  Plot the l0 norm (number of nonzeros) of the coefficients that lasso produces as you vary thestrength of regularization parameter alpha."},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas = [1, 0.5, 0.25, 0.1, 0.005, 0.001, 0.0005]\ncoef_alpha = [0, 0, 0, 0, 0, 0, 0]\nfor i in range(0,7):\n    model_las = Lasso(alpha = alphas[i]).fit(X_train, y)\n    coef = pd.Series(model_las.coef_, index = X_train.columns)\n    coef_alpha[i] = sum(coef != 0)\n    \ndf = pd.DataFrame({\"alpha\":alphas, \"l0_norm\":coef_alpha})\n\nprint(df)\n\nplt.plot('alpha', 'l0_norm', data = df)\nplt.xlabel('alpha')\nplt.ylabel('l0_norm')\nplt.xscale('log')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5.  Add the outputs of your models as features and train a ridge regression on all the featuresplus the model outputs.  Be careful not to overfit.  What score can you get?\n\nScore: 0.12495"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_y = model_ridge.predict(X_train)\nlasso_y = model_lasso.predict(X_train)\nnew_y = pd.DataFrame({\"ridge\":ridge_y, \"lasso\":lasso_y})\n\nX_train = pd.concat([X_train, new_y], axis=1)\n\nmodel_ridge2 = RidgeCV(alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]).fit(X_train, y)\nprint(\"Ridge rmse mean: \", rmse_cv(model_ridge2).mean())\nprint(\"Optimal Ridge Alpha: \", model_ridge2.alpha_)\n\nridge_y = model_ridge.predict(X_test)\nlasso_y = model_lasso.predict(X_test)\nnew_y = pd.DataFrame({\"ridge\":ridge_y, \"lasso\":lasso_y})\n\nX_test = pd.concat([X_test, new_y], axis=1)\n\nout = pd.DataFrame({\"SalePrice\":np.expm1(model_ridge2.predict(X_test))})\ni = pd.DataFrame({\"Id\":test[\"Id\"]})\nout = pd.concat([i, out], axis=1)\nout.to_csv(\"BootstrapRidge.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6.  Train a gradient boosting regression, e.g., using XGBoost.  What score can you get just froma single XGB? (you will need to optimize over its parameters).  XGB is a great friend to allgood Kagglers!\n\n\nAttempts in chronological order:\n\nUntuned: 0.14239\nEta=0.1: 0.13468\nFully tuned: 0.12878"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=1000, max_depth=6,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, scale_pos_weight=1, seed=27)\nxgb_model.fit(X_train, y)\n\nout = pd.DataFrame({\"SalePrice\":np.expm1(xgb_model.predict(X_test))})\ni = pd.DataFrame({\"Id\":test[\"Id\"]})\nout = pd.concat([i, out], axis=1)\nout.to_csv(\"XGB.csv\", index=False)\n\ncv_folds=5\nearly_stopping_rounds=50\n    \n# get optimal number of estimators\nxgb_param = xgb_model.get_xgb_params()\nxgtrain = xgb.DMatrix(X_train, label=y)\ncvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=xgb_model.get_params()['n_estimators'], nfold=cv_folds, early_stopping_rounds=early_stopping_rounds)\nxgb_model.set_params(n_estimators=cvresult.shape[0])\n\n#n_estimators optimal at 186 with eta = 0.1, 92 with eta 0.2\n    \nxgb_model.fit(X_train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_test1 = {\n 'max_depth':range(3,10,2),\n 'min_child_weight':range(1,6,2)\n}\n\ngsearch1 = GridSearchCV(estimator = xgb.XGBRegressor( learning_rate =0.1, n_estimators=186, max_depth=5,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, scale_pos_weight=1, seed=27), \n param_grid = param_test1, scoring='accuracy',n_jobs=-1, cv=5)\ngsearch1.fit(X_train, y)\ngsearch1.best_params_, gsearch1.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best max_depth: 3, min_child_weight:1\n\ngsearch1.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_test3 = {\n 'gamma':[i/10.0 for i in range(0,5)]\n}\ngsearch3 = GridSearchCV(estimator = xgb.XGBRegressor( learning_rate =0.1, n_estimators=186, max_depth=3,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, scale_pos_weight=1,seed=27), \n param_grid = param_test3, scoring='accuracy',n_jobs=-1, cv=5)\ngsearch3.fit(X_train, y)\ngsearch3.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=1000, max_depth=3,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, scale_pos_weight=1, seed=27)\n\n# get new optimal number of estimators\nxgb_param = xgb_model.get_xgb_params()\nxgtrain = xgb.DMatrix(X_train, label=y)\ncvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=xgb_model.get_params()['n_estimators'], nfold=cv_folds, early_stopping_rounds=early_stopping_rounds)\nxgb_model.set_params(n_estimators=cvresult.shape[0])\n\n# new is 320 estimators","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#optimize subsample and colsample_bytree\n#value for both is 0.6 from grid search\n\nparam_test4 = {\n 'subsample':[i/10.0 for i in range(6,10)],\n 'colsample_bytree':[i/10.0 for i in range(6,10)]\n}\ngsearch4 = GridSearchCV(estimator = xgb.XGBRegressor( learning_rate =0.1, n_estimators=320, max_depth=3,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test4, scoring='accuracy',n_jobs=-1, cv=5)\ngsearch4.fit(X_train, y)\ngsearch4.best_params_, gsearch4.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out = pd.DataFrame({\"SalePrice\":np.expm1(gsearch4.predict(X_test))})\ni = pd.DataFrame({\"Id\":test[\"Id\"]})\nout = pd.concat([i, out], axis=1)\nout.to_csv(\"XGBtuned.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7.  Do your best to get the more accurate model.  Try feature engineering and stacking manymodels.  You are allowed to use any public tool in python.  No non-python tools allowed.\n\nUpdate data preprocessing as shown at https://www.kaggle.com/adamml/how-to-be-in-top-10-for-beginner\n\nUse same model as optimized in step 6. Initial results 0.12911. Used model as shown in link above, hoping for improvement. Score 0.12112, best yet."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/house-prices-advanced-regression/train.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression/test.csv\")\n\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n\ny_train = train['SalePrice']\ntest_id = test['Id']\nall_data = pd.concat([train, test], axis=0, sort=False)\nall_data = all_data.drop(['Id', 'SalePrice'], axis=1)\n\nTotal = all_data.isnull().sum().sort_values(ascending=False)\npercent = (all_data.isnull().sum() / all_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([Total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)\n\nall_data.drop((missing_data[missing_data['Total'] > 5]).index, axis=1, inplace=True)\nprint(all_data.isnull().sum().max())\n\n# filling the numeric data\nnumeric_missed = ['BsmtFinSF1',\n                  'BsmtFinSF2',\n                  'BsmtUnfSF',\n                  'TotalBsmtSF',\n                  'BsmtFullBath',\n                  'BsmtHalfBath',\n                  'GarageArea',\n                  'GarageCars']\n\nfor feature in numeric_missed:\n    all_data[feature] = all_data[feature].fillna(0)\n    \n    #filling categorical data\ncategorical_missed = ['Exterior1st',\n                  'Exterior2nd',\n                  'SaleType',\n                  'MSZoning',\n                   'Electrical',\n                     'KitchenQual']\n\nfor feature in categorical_missed:\n    all_data[feature] = all_data[feature].fillna(all_data[feature].mode()[0])\n    \n#Fill in the remaining missing values with the values that are most common for this feature.\n\nall_data['Functional'] = all_data['Functional'].fillna('Typ')\n\nall_data.drop(['Utilities'], axis=1, inplace=True)\n\nnumeric_feats = all_data.dtypes[all_data.dtypes != 'object'].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_feats[abs(skewed_feats) > 0.5]\n\nfor feature in high_skew.index:\n    all_data[feature] = np.log1p(all_data[feature])\n\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\nall_data = pd.get_dummies(all_data)\n\nx_train =all_data[:len(y_train)]\nx_test = all_data[len(y_train):]","execution_count":26,"outputs":[{"output_type":"stream","text":"4\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=320, max_depth=3,\n min_child_weight=1, gamma=0, subsample=0.6, colsample_bytree=0.6, nthread=4, scale_pos_weight=1,seed=27)\n\nmodel.fit(x_train, y_train)\n\nprint(model.predict(x_test))\n\nout = pd.DataFrame({\"SalePrice\":np.expm1(model.predict(x_test))})\ni = pd.DataFrame({\"Id\":test[\"Id\"]})\nout = pd.concat([i, out], axis=1)\nout.to_csv(\"P7.csv\", index=False)","execution_count":21,"outputs":[{"output_type":"stream","text":"[11.712854 11.997989 12.125802 ... 12.030545 11.648226 12.325117]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"the_model = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, random_state =7, nthread = -1)\nthe_model.fit(x_train, y_train)\n\ny_predict = np.floor(np.expm1(the_model.predict(x_test)))\n\nx_train[\"Stack1\"] = np.floor(np.expm1(the_model.predict(x_train)))\nx_test[\"Stack1\"] = y_predict\n\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['SalePrice'] = y_predict\nsub.to_csv('mysubmission.csv',index=False)","execution_count":29,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  # Remove the CWD from sys.path while we load stuff.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  # This is added back by InteractiveShellApp.init_path()\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Attempt to stack previous results. Results got worse. Score of 0.12317"},{"metadata":{"trusted":true},"cell_type":"code","source":"the_model.fit(x_train, y_train)\ny_predict = np.floor(np.expm1(the_model.predict(x_test)))\n\nx_train[\"Stack2\"] = np.floor(np.expm1(the_model.predict(x_train)))\nx_test[\"Stack2\"] = y_predict\n\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['SalePrice'] = y_predict\nsub.to_csv('stacked.csv',index=False)","execution_count":30,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  after removing the cwd from sys.path.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Stacked XGB best results into best lasso; worse results than XGB, or lasso on its own. Score of 0.12626"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(x_train, y_train)\ny_predict = np.floor(np.expm1(model_lasso.predict(x_test)))\n\nx_train[\"Stack3\"] = np.floor(np.expm1(model_lasso.predict(x_train)))\nx_test[\"Stack3\"] = y_predict\n\nout = pd.DataFrame({\"SalePrice\":np.expm1(model_lasso.predict(x_test))})\ni = pd.DataFrame({\"Id\":test[\"Id\"]})\nout = pd.concat([i, out], axis=1)\nout.to_csv(\"StackXGBLasso.csv\", index=False)","execution_count":31,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.905367599604107, tolerance: 0.01891259276039605\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.508116818220573, tolerance: 0.01891259276039605\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.098087698652629, tolerance: 0.01891259276039605\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.250576489117281, tolerance: 0.01891259276039605\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.0207624544492795, tolerance: 0.01800219138548882\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.684420389836465, tolerance: 0.01800219138548882\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.59193710100422, tolerance: 0.01800219138548882\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.8241388181722655, tolerance: 0.01800219138548882\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.052525660921548, tolerance: 0.018373605848561587\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.319858604565265, tolerance: 0.018373605848561587\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.640682207237158, tolerance: 0.018373605848561587\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.779238355873531, tolerance: 0.018373605848561587\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.83755214706542, tolerance: 0.019008081403702613\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.969351621882288, tolerance: 0.019008081403702613\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.913539262014851, tolerance: 0.019008081403702613\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.095810801488009, tolerance: 0.019008081403702613\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.918784756394961, tolerance: 0.018810611883705215\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.808396115224111, tolerance: 0.018810611883705215\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.592699265145008, tolerance: 0.018810611883705215\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:528: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.783345116785105, tolerance: 0.018810611883705215\n  tol, rng, random, positive)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.570194012443461, tolerance: 0.02327976343365451\n  positive)\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  after removing the cwd from sys.path.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"\n","name":"stderr"},{"output_type":"error","ename":"ValueError","evalue":"matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 221 is different from 222)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-4a80a743286d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Stack3\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"SalePrice\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_lasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Id\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \"\"\"\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m--> 222\u001b[0;31m                                dense_output=True) + self.intercept_\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n","\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 221 is different from 222)"]}]},{"metadata":{},"cell_type":"markdown","source":"All models stacked. Score 0.125."},{"metadata":{"trusted":true},"cell_type":"code","source":"the_model.fit(x_train, y_train)\ny_predict = np.floor(np.expm1(the_model.predict(x_test)))\n\nx_train[\"Stack4\"] = np.floor(np.expm1(the_model.predict(x_train)))\nx_test[\"Stack4\"] = y_predict\n\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['SalePrice'] = y_predict\nsub.to_csv('superstacked.csv',index=False)","execution_count":32,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  after removing the cwd from sys.path.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"\n","name":"stderr"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}